{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacksonchen1998/NTUT-Machine-Leraning-Course/blob/main/Homework_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "UC Irvine has a large repository for various kinds of data. \n",
        "\n",
        "In this problem, you are asked to use the iris dataset (https://archive.ics.uci.edu/ml/datasets/Iris) to perform the experiments. \n",
        "\n",
        "To begin one trial, randomly draw 70% of the instances for training and the rest for testing. \n",
        "\n",
        "Note: you can directly import iris dataset by using sklearn without downloading from the UC Irvine repository. \n",
        "\n",
        "Using GMM with 2 mixtures instead. \n",
        "\n",
        "The GMM tools are supported in sklearn. \n",
        "\n",
        "Remember to use one model per class. Of the k-NN, Naïve Bayes, and GMM classifiers, which one has the best accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tr51oQaBrwxM"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets # import dataset\n",
        "from sklearn.mixture import GaussianMixture # import GMM\n",
        "from sklearn.model_selection import train_test_split # import split function \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xCdJdsdFxpXh"
      },
      "outputs": [],
      "source": [
        "colors = [\"orange\", \"blue\"]\n",
        "\n",
        "def make_ellipses(gmm, ax):\n",
        "    for n, color in enumerate(colors):\n",
        "        if gmm.covariance_type == \"full\":\n",
        "            covariances = gmm.covariances_[n][:2, :2]\n",
        "        elif gmm.covariance_type == \"tied\":\n",
        "            covariances = gmm.covariances_[:2, :2]\n",
        "        elif gmm.covariance_type == \"diag\":\n",
        "            covariances = np.diag(gmm.covariances_[n][:2])\n",
        "        elif gmm.covariance_type == \"spherical\":\n",
        "            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n",
        "        v, w = np.linalg.eigh(covariances)\n",
        "        u = w[0] / np.linalg.norm(w[0])\n",
        "        angle = np.arctan2(u[1], u[0])\n",
        "        angle = 180 * angle / np.pi  # convert to degrees\n",
        "        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\n",
        "        ell = mpl.patches.Ellipse(\n",
        "            gmm.means_[n, :2], v[0], v[1], 180 + angle, color=color\n",
        "        )\n",
        "        ell.set_clip_box(ax.bbox)\n",
        "        ell.set_alpha(0.5)\n",
        "        ax.add_artist(ell)\n",
        "        ax.set_aspect(\"equal\", \"datalim\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "TxUWC_iBx7VP",
        "outputId": "fec1a4cd-f639-40a4-806a-abfb07fd2c05"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()\n",
        "\n",
        "# Break up the dataset into non-overlapping training (70%) and testing\n",
        "# (30%) sets.\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data,iris.target,test_size=0.3)\n",
        "\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "# Try GMMs using different types of covariances.\n",
        "estimators = {\n",
        "    cov_type: GaussianMixture(\n",
        "        n_components=n_classes, covariance_type=cov_type, max_iter=20, random_state=0\n",
        "    )\n",
        "    for cov_type in [\"spherical\", \"diag\", \"tied\", \"full\"]\n",
        "}\n",
        "\n",
        "n_estimators = len(estimators)\n",
        "\n",
        "plt.figure(figsize=(3 * n_estimators // 2, 6))\n",
        "plt.subplots_adjust(\n",
        "    bottom=0.01, top=0.95, hspace=0.15, wspace=0.05, left=0.01, right=0.99\n",
        ")\n",
        "\n",
        "\n",
        "for index, (name, estimator) in enumerate(estimators.items()):\n",
        "    # Since we have class labels for the training data, we can\n",
        "    # initialize the GMM parameters in a supervised manner.\n",
        "    estimator.means_init = np.array(\n",
        "        [X_train[y_train == i].mean(axis=0) for i in range(n_classes)]\n",
        "    )\n",
        "\n",
        "    # Train the other parameters using the EM algorithm.\n",
        "    estimator.fit(X_train)\n",
        "\n",
        "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
        "    make_ellipses(estimator, h)\n",
        "\n",
        "    for n, color in enumerate(colors):\n",
        "        data = iris.data[iris.target == n]\n",
        "        plt.scatter(\n",
        "            data[:, 0], data[:, 1], s=0.8, color=color, label=iris.target_names[n]\n",
        "        )\n",
        "    # Plot the test data with crosses\n",
        "    for n, color in enumerate(colors):\n",
        "        data = X_test[y_test == n]\n",
        "        plt.scatter(data[:, 0], data[:, 1], marker=\"x\", color=color)\n",
        "\n",
        "    y_train_pred = estimator.predict(X_train)\n",
        "    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n",
        "    plt.text(0.05, 0.9, \"Train accuracy: %.1f\" % train_accuracy, transform=h.transAxes)\n",
        "\n",
        "    y_test_pred = estimator.predict(X_test)\n",
        "    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n",
        "    plt.text(0.05, 0.8, \"Test accuracy: %.1f\" % test_accuracy, transform=h.transAxes)\n",
        "\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.title(name)\n",
        "\n",
        "plt.legend(scatterpoints=1, loc=\"lower right\", prop=dict(size=12))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this problem, you are asked to perform the wrapper-type feature selection using the Naïve Bayes classifier for cancer dataset \n",
        "\n",
        "(Breast Cancer Wisconsin (Original) Data Set, directly from the sklearn or downloading \n",
        "\n",
        "from https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Original%29). \n",
        "\n",
        "To simplify the problem, we just want to keep 5 attributes out of 9 (hint: one attribute is useless. Which one is it?) \n",
        "\n",
        "To begin one experiment, randomly draw 60 % of the instances from each class for training, \n",
        "\n",
        "and 20% from each class for finding the best 5 attributes. \n",
        "\n",
        "Once the feature selection is complete, use the rest 20% for testing to obtain the accuracy. \n",
        "\n",
        "Repeat the selection 10 times to get the average accuracy. \n",
        "\n",
        "You need to deal with missing attributes. \n",
        "\n",
        "Compare the obtained accuracy with the same type of model trained with the full set of 9 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://drive.google.com/file/d/1foB8RSromI9rH_5nRaHubkr12zkElmmL/view?usp=sharing\"\n",
        "url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "df = pd.read_csv(url, header=None) #file contains no header info\n",
        "print(f\"Read in {len(df)} rows\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.replace(\"?\", 10000, inplace=True) #10,000 is way beyond the range of columns provided so acts as an outlier\n",
        "df.drop([0], 1, inplace=True)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import feature_selection as fs\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = np.array(df.drop([10], 1)) #last column contains label, so ignore it when creating X\n",
        "y = np.array(df[10]) #last column contains label which is our y\n",
        "\n",
        "X_new = fs.SelectKBest(fs.chi2,k=5).fit_transform(X, y) # Using feature selection\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for _ in range(10):\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_new, y, test_size=0.4, random_state=43)\n",
        "  X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=43)\n",
        "\n",
        "  gnb_model = GaussianNB()\n",
        "  y_result = gnb_model.fit(X_train, y_train).predict(X_test)\n",
        "  sum += accuracy_score(y_test, y_result)\n",
        "\n",
        "print(f\"Accuracy of Gaussian Naive Bayes Classifier is:{sum / 10}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import feature_selection as fs\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "X = np.array(df.drop([10], 1)) #last column contains label, so ignore it when creating X\n",
        "y = np.array(df[10]) #last column contains label which is our y\n",
        "\n",
        "sum = 0 # Original one using Naive Bayes Classify\n",
        "\n",
        "for _ in range(10):\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=43)\n",
        "  X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=43)\n",
        "\n",
        "  gnb_model = GaussianNB()\n",
        "  y_result = gnb_model.fit(X_train, y_train).predict(X_test)\n",
        "  sum += accuracy_score(y_test, y_result)\n",
        "\n",
        "print(f\"Accuracy of Gaussian Naive Bayes Classifier is:{sum / 10}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPBgsfB/MP41puNgywA338N",
      "include_colab_link": true,
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
